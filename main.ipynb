{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d333515a",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "به فیلم چند میدی؟\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99450f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ead86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "imdb_dataset = load_dataset(\"csv\", data_files=\"train.csv\", split='train').remove_columns('Movie_ID').train_test_split(0.20)\n",
    "test_valid = imdb_dataset['test'].train_test_split(0.30)\n",
    "imdb_dataset = DatasetDict({\n",
    "    'train': imdb_dataset['train'],\n",
    "    'test': test_valid['train'],\n",
    "    'valid': test_valid['test']})\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Review\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3831a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(imdb_dataset[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_encoded = imdb_dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95800bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "# Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy().tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Rating\"])\n",
    "emotions_hidden = rating_encoded.map(extract_hidden_states, batched=True, batch_size=16)\n",
    "emotions_hidden[\"train\"].column_names\n",
    "# emotion_hidden = load_from_disk(\"emotion-hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27161ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_numpy(length, train_test_valid, hidden_Rating):\n",
    "#     X = []\n",
    "#     for i in range(length):\n",
    "#         X.append(np.array(emotions_hidden[train_test_valid][hidden_Rating][i].cpu().numpy()))\n",
    "#     X= np.array(X)\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e90dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = convert_to_numpy(len(emotions_hidden[\"train\"][\"hidden_state\"]), \"train\", \"hidden_state\")\n",
    "# y_train = convert_to_numpy(len(emotions_hidden[\"train\"][\"Rating\"]), \"train\", \"Rating\")\n",
    "# X_test = convert_to_numpy(len(emotions_hidden[\"test\"][\"hidden_state\"]), \"test\", \"hidden_state\")\n",
    "# y_test = convert_to_numpy(len(emotions_hidden[\"test\"][\"Rating\"]), \"test\", \"Rating\")\n",
    "# X_valid = convert_to_numpy(len(emotions_hidden[\"valid\"][\"hidden_state\"]), \"valid\", \"hidden_state\")\n",
    "# y_valid = convert_to_numpy(len(emotions_hidden[\"valid\"][\"Rating\"]), \"valid\", \"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.DataFrame(columns=['X_test', 'y_test'])\n",
    "# df_train['X_train'] = X_train.tolist()\n",
    "# df_train['y_train'] = y_train.tolist()\n",
    "# df_train.to_csv('X,y_train.csv', index=False)\n",
    "\n",
    "# df_test = pd.DataFrame(columns=['X_test', 'y_test'])\n",
    "# df_test['X_test'] = X_test.tolist()\n",
    "# df_test['y_test'] = y_test.tolist()\n",
    "# df_test.to_csv('X,y_test.csv', index=False)\n",
    "\n",
    "\n",
    "# df_valid = pd.DataFrame(columns=['X_valid', 'y_valid'])\n",
    "# df_valid['X_valid'] = X_valid.tolist()\n",
    "# df_valid['y_valid'] = y_valid.tolist()\n",
    "# df_valid.to_csv('X,y_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da953e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RELOADING\n",
    "# df_train = pd.read_csv('X,y_train.csv')\n",
    "# df_test = pd.read_csv('X,y_test.csv')\n",
    "# df_valid = pd.read_csv('X,y_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df_train['X_train'].to_numpy()\n",
    "# y_train = df_train['y_train'].to_numpy()\n",
    "# X_test = df_test['X_test'].to_numpy()\n",
    "# y_test = df_test['y_test'].to_numpy()\n",
    "# X_valid = df_valid['X_valid'].to_numpy()\n",
    "# y_valid = df_valid['y_valid'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b73c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train[2]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(20,10))\n",
    "# axes = axes.flatten()\n",
    "# cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\", \"pink\", \"YlOrBr\", \"YlGn\", \"YlGnBu\"]\n",
    "# labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "#     df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "#     axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "#     gridsize=20, linewidths=(0,))\n",
    "#     axes[i].set_title(label)\n",
    "#     axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "num_labels = 10\n",
    "model = (AutoModelForSequenceClassification\n",
    ".from_pretrained(model_ckpt, num_labels=num_labels)\n",
    ".to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c476589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cac1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# rating_encoded = rating_encoded.rename_column(\"Rating\", \"labels\")\n",
    "rating_encoded = rating_encoded.cast_column(\"labels\", ClassLabel(num_classes=10))\n",
    "rating_encoded['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for split in rating_encoded.keys():\n",
    "    rating_encoded[split] = rating_encoded[split].filter(\n",
    "        lambda x: not torch.isnan(x[\"labels\"]).any().item()  # keep only rows without NaN\n",
    "    )\n",
    "\n",
    "print(rating_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_encoded = rating_encoded.map(lambda e: {\"labels\": int(e[\"labels\"]) - 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels from tensors -> int\n",
    "def to_int(example):\n",
    "    return {\"labels\": int(example[\"labels\"].item())}  # .item() gets scalar from tensor\n",
    "\n",
    "rating_encoded = rating_encoded.map(to_int)\n",
    "print(rating_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_encoded['train']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2319ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_encoded[\"train\"]['labels'][0] = rating_encoded[\"train\"]['labels'][0].item()\n",
    "rating_encoded[\"train\"]['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "batch_size = 32\n",
    "logging_steps = len(rating_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    "    log_level=\"error\")\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "compute_metrics=compute_metrics,\n",
    "train_dataset=rating_encoded[\"train\"],\n",
    "eval_dataset=rating_encoded[\"valid\"],\n",
    "tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# train_data[\"Rating\"].value_counts(ascending=True).plot.barh()\n",
    "# plt.title(\"Frequency of Classes\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637db567",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sentence = max([len(s.split()) for s in train_dataset['Review']])   \n",
    "max_len_word = 0\n",
    "for review in (train_dataset['Review']):\n",
    "    for word in review.split():\n",
    "        if len(word) > max_len_word:\n",
    "            max_len_word = len(word)\n",
    "    \n",
    "max_len_sentence, max_len_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test samples\n",
    "submission = pd.DataFrame()\n",
    "submission\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
